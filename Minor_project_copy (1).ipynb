{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQZ0l5bfh3qy"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TPZAc-gUiTu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort images and reads the image using OpenCV's imread() function and stores it in the arr variable and then Resizes the image to a shape of (256, 256) using bilinear interpolation and stores it in the arr2 variable and append processed images to x.\n",
        "from PIL import Image\n",
        "import os\n",
        "image_names=os.listdir('/content/drive/MyDrive/hazeimage/hazy')\n",
        "folder_path = '/content/drive/MyDrive/hazeimage/hazy'\n",
        "image_names.sort()\n",
        "x=[]\n",
        "for image_file in image_names:\n",
        "    image_path = os.path.join(folder_path, image_file)\n",
        "    arr=cv2.imread(image_path)\n",
        "    #arr=cv2.cvtColor(arr,cv2.COLOR_BGR2RGB)\n",
        "    #print(arr.shape)\n",
        "    arr2=cv2.resize(arr,(256,256),interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    x.append(arr2)"
      ],
      "metadata": {
        "id": "c2cgc0cqiJtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "image_names=os.listdir('/content/drive/MyDrive/hazeimage/GT')\n",
        "folder_path = '/content/drive/MyDrive/hazeimage/GT'\n",
        "image_names.sort()\n",
        "y=[]\n",
        "for image_file in image_names:\n",
        "    image_path = os.path.join(folder_path, image_file)\n",
        "    arr=cv2.imread(image_path)\n",
        "    #arr=cv2.cvtColor(arr,cv2.COLOR_BGR2RGB)\n",
        "    arr2=cv2.resize(arr,(256,256),interpolation=cv2.INTER_LINEAR)\n",
        "    y.append(arr2)"
      ],
      "metadata": {
        "id": "3VG-8OEciL1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array(x)/255\n",
        "y=np.array(y)/255\n",
        "#Converts the list x containing processed images into a NumPy array, and then divides each pixel value by 255. This operation scales the pixel values to the range [0, 1], which is a common normalization technique for image data.\n",
        "#By normalizing the pixel values to the range [0, 1], you ensure that the input data is scaled appropriately for training machine learning models, potentially improving convergence and performance during training."
      ],
      "metadata": {
        "id": "0ecagRsmiQRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape\n",
        "#(256 pixels height,weidth and 3 color channels (RGB))"
      ],
      "metadata": {
        "id": "m7iCTbbmiRNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.5)\n",
        "# xtrain: Contains the training samples of the input data.\n",
        "# xtest: Contains the testing samples of the input data.\n",
        "# ytrain: Contains the corresponding ground truth (GT) training labels.\n",
        "# ytest: Contains the corresponding GT testing labels."
      ],
      "metadata": {
        "id": "l-9869-BiVQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain=np.array(xtrain)\n",
        "ytrain=np.array(ytrain)\n",
        "xtest=np.array(xtest)\n",
        "ytest=np.array(ytest)\n",
        "#converting each list into array"
      ],
      "metadata": {
        "id": "cQKBLPr-ibPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x[10])\n",
        "#to display 11th element from the x array"
      ],
      "metadata": {
        "id": "W2tjJnIoidbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.layers import Conv2D,MaxPooling2D,Input,UpSampling2D, BatchNormalization\n",
        "# from tensorflow.keras.models import Model\n",
        "\n",
        "# Imports various layers used for building convolutional neural networks (CNNs) and other types of neural network architectures.\n",
        "# Conv2D: Convolutional layer for 2D spatial convolution.\n",
        "# MaxPooling2D: Max pooling operation for spatial data.\n",
        "# Input: Input layer for defining input shape.\n",
        "# UpSampling2D: UpSampling layer for 2D inputs.\n",
        "# Dropout: Dropout layer for regularization.\n",
        "# BatchNormalization: Batch normalization layer to improve the training speed and stability of neural networks.\n",
        "# Conv2DTranspose: Transposed convolution layer for upsampling."
      ],
      "metadata": {
        "id": "z6TrCaj8ifIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Functions : Mean Squared Error (MSE) (commonly used loss function in regression tasks, including image reconstruction tasks like image dehazing)\n",
        "# Optimization Algorithms : Adam (Adam adapts the learning rates for each parameter during training, which can lead to faster convergence and better performance compared to using a fixed learning rate.)\n",
        "# Activation Function :  Relu (introduces non-linearity to the network, allowing it to learn complex relationships between input features and output predictions, ReLU is simple to compute and does not involve complex mathematical operations. addresses the vanishing gradient problem )\n",
        "# Training Hyperparameters : Learning rate , batch size, number of epochs(1000)\n",
        "# Number of layers:\n",
        "#              Convolutional layers: 10 (including the repeated blocks)\n",
        "#              Max pooling layers: 5\n",
        "#              Upsampling layers: 5\n",
        "#              Batch normalisation :10\n",
        "#          Total number of layers = Convolutional layers + Max pooling layers + Upsampling layers\n",
        "#                                                         = 10 + 5 + 5 + 10= 30\n",
        "\n",
        "\n",
        "#encoder start\n",
        "\n",
        "# inputs=Input(shape=(256,256,3))\n",
        "\n",
        "# con1=Conv2D(32,5,activation='relu',padding='same')(inputs)\n",
        "# #con1=BatchNormalization()(con1)\n",
        "# maxp=MaxPooling2D(2,padding='same')(con1)\n",
        "# bn=BatchNormalization()(maxp)\n",
        "\n",
        "# con2=Conv2D(64,5,activation='relu',padding='same')(bn)\n",
        "# #con2=BatchNormalization()(con2)\n",
        "# maxp1=MaxPooling2D(2,padding='same')(con2)\n",
        "# bn=BatchNormalization()(maxp1)\n",
        "\n",
        "# con3=Conv2D(80,5,activation='relu',padding='same')(bn)\n",
        "# #con3=BatchNormalization()(con3)\n",
        "# maxp2=MaxPooling2D(2,padding='same')(con3)\n",
        "# bn=BatchNormalization()(maxp2)\n",
        "\n",
        "# con3=Conv2D(128,5,activation='relu',padding='same')(bn)\n",
        "# #con3=BatchNormalization()(con3)\n",
        "# maxp2=MaxPooling2D(2,padding='same')(con3)\n",
        "# bn=BatchNormalization()(maxp2)\n",
        "\n",
        "# con3=Conv2D(256,5,activation='relu',padding='same')(bn)\n",
        "# #con3=BatchNormalization()(con3)\n",
        "# maxp2=MaxPooling2D(2,padding='same')(con3)\n",
        "# bn=BatchNormalization()(maxp2)\n",
        "\n",
        "# #encoder end\n",
        "\n",
        "# #decoder start\n",
        "\n",
        "# con4=Conv2D(256,5,activation='relu',padding='same')(bn)\n",
        "# con4=BatchNormalization()(con4)\n",
        "# up=UpSampling2D(2)(con4)\n",
        "\n",
        "# con4=Conv2D(128,5,activation='relu',padding='same')(up)\n",
        "# con4=BatchNormalization()(con4)\n",
        "# up=UpSampling2D(2)(con4)\n",
        "\n",
        "# con4=Conv2D(80,5,activation='relu',padding='same')(up)\n",
        "# con4=BatchNormalization()(con4)\n",
        "# up=UpSampling2D(2)(con4)\n",
        "\n",
        "# con5=Conv2D(64,5,activation='relu',padding='same')(up)\n",
        "# con5=BatchNormalization()(con5)\n",
        "# up1=UpSampling2D(2)(con5)\n",
        "\n",
        "# con6=Conv2D(32,5,activation='relu',padding='same')(up1)\n",
        "# con6=BatchNormalization()(con6)\n",
        "# up2=UpSampling2D(2)(con6)\n",
        "\n",
        "# out=Conv2D(3,4,activation='sigmoid',padding='same')(up2)\n",
        "\n",
        "# #decoder end\n",
        "\n",
        "# model=Model(inputs=inputs,outputs=out)\n",
        "# model.summary()\n",
        "\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, UpSampling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Define the model function\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Encoder part\n",
        "    model.add(Conv2D(32, 5, activation='relu', padding='same', input_shape=(256, 256, 3)))\n",
        "    model.add(MaxPooling2D(2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(64, 5, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(80, 5, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(128, 5, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Conv2D(256, 5, activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(2, padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    # Decoder part\n",
        "\n",
        "    model.add(Conv2D(256, 5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(UpSampling2D(2))\n",
        "\n",
        "    model.add(Conv2D(128, 5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(UpSampling2D(2))\n",
        "\n",
        "    model.add(Conv2D(80, 5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(UpSampling2D(2))\n",
        "\n",
        "    model.add(Conv2D(64, 5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(UpSampling2D(2))\n",
        "\n",
        "    model.add(Conv2D(32, 5, activation='relu', padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(UpSampling2D(2))\n",
        "# Decoder end\n",
        "    model.add(Conv2D(3, 4, activation='sigmoid', padding='same'))\n",
        "\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "model = create_model()\n",
        "\n",
        "# Print model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "jBoVp1D2ihIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "X-IpxfXUijk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "sBbvopNFgRQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histr=model.fit(x,y,epochs=1000,validation_data=[xtest,ytest],batch_size=5)\n",
        "#55/5 ->11 batches"
      ],
      "metadata": {
        "id": "XLOLIuSYinps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the model compilation to include the 'accuracy' metric\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "# Train the model again\n",
        "histr = model.fit(x, y, epochs=1000, validation_data=[xtest, ytest], batch_size=5)\n",
        "\n",
        "# Access the training accuracy from the history object\n",
        "training_accuracy = histr.history['accuracy']\n",
        "\n",
        "# Access the validation accuracy from the history object\n",
        "validation_accuracy = histr.history['val_accuracy']\n"
      ],
      "metadata": {
        "id": "gb_xKBAodY5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model_without_dropout.keras')"
      ],
      "metadata": {
        "id": "HlKIttWfip89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Assuming you have already trained your model and stored the training history in histr\n",
        "# Accessing training accuracy\n",
        "training_accuracy = histr.history['accuracy']\n",
        "# Accessing validation accuracy\n",
        "validation_accuracy = histr.history['val_accuracy']\n",
        "# Generating the epoch numbers for the x-axis\n",
        "epochs = range(1, len(training_accuracy) + 1)\n",
        "# Plotting the training and validation accuracy\n",
        "plt.plot(epochs, training_accuracy, 'r', label='Training Accuracy')\n",
        "plt.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BRcShvz-l_ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation loss vs epochs\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(histr.history['val_loss'])\n"
      ],
      "metadata": {
        "id": "wtxOFdLPl_br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training loss vs epochs\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(histr.history['loss'])\n"
      ],
      "metadata": {
        "id": "6gORaLAfi1XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to view an image before dehazing\n",
        "arr=cv2.imread('/content/drive/MyDrive/globeindiasmog_lrg.jpg')\n",
        "#arr=cv2.cvtColor(arr,cv2.COLOR_BGR2RGB)\n",
        "arr2=cv2.resize(arr,(256,256),interpolation=cv2.INTER_LINEAR)\n",
        "arr2=np.array([arr2])/255\n",
        "plt.title('image before')\n",
        "plt.imshow(arr2[0])"
      ],
      "metadata": {
        "id": "yn49PJdii4KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to view an image after dehazing\n",
        "arr=cv2.imread('/content/drive/MyDrive/globeindiasmog_lrg.jpg')\n",
        "#arr=cv2.cvtColor(arr,cv2.COLOR_BGR2RGB)\n",
        "arr2=cv2.resize(arr,(256,256),interpolation=cv2.INTER_LINEAR)\n",
        "arr2=np.array([arr2])/255\n",
        "#arr2=arr2/255\n",
        "prerd=model.predict(arr2)\n",
        "import matplotlib.pyplot as plt\n",
        "prerd=prerd\n",
        "plt.title('image after dehazing')\n",
        "plt.imshow(prerd[0])"
      ],
      "metadata": {
        "id": "x5Qi1DbdnnXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training accuracy\n",
        "training_accuracy = histr.history['accuracy']"
      ],
      "metadata": {
        "id": "f5iH02YQovD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(training_accuracy)"
      ],
      "metadata": {
        "id": "fuwr228DpmnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation accuracy\n",
        "validation_accuracy = histr.history['val_accuracy']\n"
      ],
      "metadata": {
        "id": "mSDRrL2hnpjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(validation_accuracy)"
      ],
      "metadata": {
        "id": "K0brN09RoYvj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}